\documentclass[9pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,relsize}
\usepackage{color,graphicx,overpic}
\usepackage{enumitem}

% Turn off header and footer
\pagestyle{empty}
\geometry{top=.12in,left=.25in,right=.25in,bottom=.12in}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{\z@}{2ex}{1ex}
                       {\normalfont\normalsize\bfseries\textit}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{\z@}{2ex}{0.5ex}
                          {\normalfont\small\bfseries}}
\makeatother

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

\input{math_commands.tex}

\newcommand{\ruler}{\\\rule{\columnwidth}{0.25pt}\\}
\newcommand{\uruler}{\rule{\columnwidth}{0.25pt}\\}

\newcommand{\itemizebegin}{\begin{itemize}[noitemsep,topsep=0pt]}
\newcommand{\itemizeend}{\end{itemize}}

\newcommand{\enumbegin}{\begin{enumerate}[noitemsep,topsep=0pt]}
\newcommand{\enumend}{\end{enumerate}}

\DeclareMathOperator*{\argmin}{arg\,min}



% -----------------------------------------------------------------------
\begin{document}
\raggedright
\footnotesize
\begin{multicols*}{3}
% multicol parameters
% These lengths are set only within the two main columns
% \setlength{\columnseprule}{0.1pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{0pt}



\section{Unconstrained Optimization Algorithms}
\textbf{General Ideas}
\itemizebegin
\item Select step size $s_k$ and direction $v_k$ such that $f_0(x_k + s_k v_k)$ is substantially smaller
than $f_0(x_k)$.
\item $v_k$ is usually selected such that $\nabla f_0(x_k)^\top v_k < 0$
\item First order descent
\itemizeend
\uruler
\textbf{Armijo backtracking step size rule}
\enumbegin
\item Chosse $\alpha, \beta \in (0, 1)$, set $s = 1$
\item If $f_0(x_k + s v_k) \le f_0(x_k) + s \alpha \nabla f_0(x_k) ^ \top v_k$, then return $s_k = s$.
\item Set $s = \beta s$ and return to step 2.
\enumend
\uruler
\textbf{Newton-type methods}
\itemizebegin
\item Newton's method: $v_k = -\nabla^2 f_0(x_k)^{-1} \nabla f_0(x_k)$
\item Interpretation: Minimizing second order approximation: $\min_x f_0(x_k) + \nabla f_0(x_k)^\top x + \frac{1}{2} x^\top \nabla^2 f_0(x_k) x$
\item Quasi-Newton and variable-metric: $v_k = -H_k \nabla f_0(x_k)$
\item Combined with any step size rule
\itemizeend

% --------------------------------

\section{Constrained Optimization Algorithms}
\textbf{Methods}
\itemizebegin
\item \textbf{Projected Decent:} Use a direction $v_k$ from unconstrained algorithms and compute $x^\prime_{k+1} = x_k + s_k v_k$ and then project $x^\prime_{k+1}$ onto the feasible set.
\item Construct an unconstrained problem that approximates the original problem.
\itemizeend
\uruler
\textbf{Barrier method}
\itemizebegin
\item Consider the problem $\min_x f_0(x)$ subject to $f_i(x) \le 0$ for $i = 1...m$.
\item Define the approximate problem for some parameter $t \ge 0$: $\min_x f_0(x) + \frac{1}{t}\phi (x)$, where the barrier function $\phi (x)$ tends to $\infty$ as $x$ tends to the boundary of the feasible set from inside.
\item We can use the logarithmic barrier function: $\phi (x) = -\sum_{i=1}^{m}{log(-f_i(x))}$
\item If $f_0$, $f_i$ are convex, the approximated problem is convex
\item The KKT condition is $t \nabla f_0(x) + \sum_{i=1}^{m}{\frac{1}{-f_i(x)} \nabla f_i(x)} = 0$
\item The approximation error: $0 \le f_0(x^*(t)) - p^* \le \frac{m}{t}$
\itemizeend

\section{Machine Learning}
\textbf{Linear Model}
\itemizebegin
\item Linear model: $y(x) = w^\top \phi (x) + e(x)$, where $\phi(x)$ is a feature map and $e(x)$ is a random error
\item Least square: $\hat{w} \in \argmin_w \sum_i{(w^\top \phi(x_i) - y_i)^2}$
\item Quantile for $\alpha \in [0, 1)$: $\hat{w} \in \argmin_w \frac{1}{m(1 - \alpha)} \sum_i{max\{0, w^\top \phi(x_i) - y_i\}} - \frac{1}{m} \sum_i{w^\top \phi(x_i) - y_i}$. Still a linear program.
\itemizeend
\uruler
\textbf{Support Vector Machine}
\itemizebegin
\item Classification rule: $\hat{y}(x)=sign(\phi(x)^\top x)$
\item Convex error function(hinge): $\max \{0, 1-\alpha \}$
\item Objective: $\min_w \frac{1}{m} \sum_i{\max\{0, 1-y_i \phi(x_i)^\top w\}} $
\item We can add $L_1$ or $L_2$ norm regularization on $w$
\itemizeend
\uruler
\textbf{Logistic Regression}
\itemizebegin
\item Variant of SVM with another loss function: $\min_w \frac{1}{m} \sum_i{log[1 + exp(-y_i \phi(x_i)^\top w)]}$
\item Interpretation: $P\{Y = y | X = x\} = \frac{1}{1 + exp(-y \phi(x)^\top w)}$
\itemizeend
\columnbreak

\textbf{Fisher discrimination}
\itemizebegin
\item Idea: find a direction in $\R^n$, such that the projection of data points onto the direction results in those points with positive label are from those with negative label
\item Define $A_+ \in R^{n,m_+}$ is the matrix whose column cosists of $x_i$ such that $y_i = 1$, and $A_-$ similarly for negive labeled points. The centered(mean subtracted) data matrices are
$\widetilde{A}_{\pm} = A_{\pm} (I_{m_{\pm}} - \frac{1}{m_{\pm}}\mathbf{1} \mathbf{1}^\top)$
\item The mean-squared variation of the data projected on $u$ around its centroid is:
$u^\top M u = u^\top (\frac{1}{m_-} \widetilde{A}_- \widetilde{A}_-^\top + \frac{1}{m_+} \widetilde{A}_+ \widetilde{A}_+^\top )u$
\item The goal is: $\max_{u \ne 0} \frac{(u^\top c)^2}{u^\top M u} = \min u^\top M u$ subject to $u^\top c = 1$
\item The problem is convex, hence KKT condition implies: $u = \frac{1}{c^\top M^{-1} c} M^{-1} c$
\itemizeend
\uruler
\textbf{SPCA and NNMF}
\itemizebegin
\item The original PCA problem: $\max_z z^\top \widetilde{X}\widetilde{X}^\top z $ s.t. $\|z\|_2=1$
\item In sparse PCA, we add constraint $card(z) \le k$
\item The problem leads to the rank 1 approximation problem: $\min_{p,q} \|X - p q^\top \|_F$ s.t.
$card(p) \le k, card(q) \le h$
\item One example of rank 1 approximation is: $\min_{p \ge 0,q \ge 0} \|X - p q^\top \|_F$ s.t.
$card(p) \le k, card(q) \le h$. A standard solution is coordinate descent.
\itemizeend

\section{Finance}
\textbf{Mean-Variance (Markowitz) Models}
\itemizebegin
\item $x \in \R^n$ is the wealth allocation in n groups, $r$ is the random vector of unit wealth return for each group. $\hat{r} = E[r], \Sigma = Cov(r)$
\item Hence, $\hat{r}^\top x$ is the expected return and $x^\top \Sigma x$ is the variance of the portfolio.
\item The problems are usually maximize return with variance constraints and minimize variance, with return constraints.
\item Buget constraint: $x^\top \mathbf{1} \le b$, no short-selling: $x \ge 0$
\item Sector bounds: $\sum_{i \in S}{x_i} \le \alpha x^\top \mathbf{1}$ for some $\alpha \in (0, 1)$ and sector $S$.
\item Diversification: sum of $k$ largest $x_i$ must not exceed $\eta x^\top \mathbf{1}$ for $\eta \in (0, 1)$.
This is equivalent to $kt + s^\top \mathbf{1} \le \eta x^\top \mathbf{1}$, $s \ge 0, x - t\mathbf{1} \le s$ where $t \in \R$ and $s \in \R^n$ are additional variables.
\item The Sharpe Ratio of a portfolio is $(\hat{r}^\top x - r_f) / \sqrt{x^\top \Sigma x}$, where $r_f$ is the
return of a risk-free asset.
\item If $\hat{r}^\top x > r_f$ and $\Sigma \succ 0$, then the portfolio that maximizes SR corresponds to a
point on the efficient frontier. Max SR can be formulated as SOCP.
\itemizeend
\uruler
\textbf{Value at Risk and Conditional Value at Risk}
\itemizebegin
\item The value-of-risk of a portfolio $r^\top x$ at probability level $\alpha \in (0, 1)$ is defined as
$VaR_{\alpha}(r^\top x) = -F^{-1}(\alpha)$, where $F$ is the CDF of $r^\top x$.
\item If $r$ is Gaussian with mean $\hat{r}$ and covariance $\Sigma$, we have $VaR_{\alpha}(r^\top x) \le \gamma$ is equvalent as SOC constraint $\Phi^{-1}(1-\alpha)\|\Sigma^{\frac{1}{2}}\|_2 \le \gamma + \hat{r}^\top x$
\itemizeend

\columnbreak
% --------------------------
% TODO: Add more finance

\section{Epi-Spline Curve Fitting}
\textbf{Epi-splines}
\itemizebegin
\item Epi-spline: Left continous, piece-wise polynomial
\item On the $k^{th}$ segment, $q^{k}(x) = \sum_{i=0}^{p}{a_i^k x^i}$. Hence if we split the domain into $N$ segments with degree $p$ polynomial, our total number of parameters would be $N (p+1)$
\item Continuity constraint: Let $m_1 ... m_{N-1}$ be the mesh points, we have $q^k(m_k) = q^{k+1}(m_k)$ for $k = 1...N-1$.
\item Continuous differentiability: Continuty constraint plus $(q^k(m_k))^\prime = (q^{k+1}(m_k))^\prime$ for $k = 1...N-1$.
\item Fixed values: $q^{k}(x) = s(k)$
\item Monotonicity (non-increasing): $(q^k(x))^\prime \le 0$ for $x \in (m_{k-1}, m_k)$, and
$q^k(m_k) \ge q^{k+1}(m_{k+1})$. The second requirement is satisfied with continuity.
\item Convexity: Continuty constraint and $(q^k(x))^{\prime\prime} \ge 0$, $(q^k(m_k))^\prime \le (q^{k+1}(m_{k+1}))^\prime$ for $k = 1...N-1$.
\item All constraints above are linear.
\itemizeend

\section{Optimization in Control}
\textbf{Continuous LTI Systems}
\itemizebegin
\item A continuous LTI system is described by a system ODEs: $\dot{x}(t) = A x(t) + B u(t)$, $y(t) = C x(t)$
\item The solution is: $x(t) = e^{A(t - t_0)}x(t_0) + \int_{t_0}^{t}{e^{A(t-\tau)} B u(\tau) d \tau}$,
$t \ge t_0$
\itemizeend
\uruler
\textbf{Discrete-time LTI System and Discretization}
\itemizebegin
\item We discretize the time interval $t = k\Delta$ for $k = ... -1, 0, 1, ...$ and DTLTI system can be described as $x(k+1) = A x(k) + B u(k), y(k) = C x(k)$
\item The solution is: $x(k) = A^{k-k_0} x(k_0) + \sum_{i=k_0}^{k-1}{A^{k-i-1} B u(i)}$ for $k >= k_0$.
\item Assuming constant control input during one time interval, i.e. $u(t) = u(k\Delta), \forall t \in [k\Delta, (k+1)\Delta)$, we can discretize a CTLTI system by letting $A_{\Delta}=e^{A\Delta}, B_{\Delta}=\int_0^{\Delta}{e^{A\tau}d\tau}$. The we get the system $x(k+1) = A_{\Delta} x(k) + B_{\Delta} u(k)$
\itemizeend
\uruler
\textbf{Optimization-based Control Synthesis}







\end{multicols*}
\end{document}
